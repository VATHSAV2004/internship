{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcc7cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University Name</th>\n",
       "      <th>Student Name</th>\n",
       "      <th>Project Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Project Category/Field</th>\n",
       "      <th>Project Supervisor/Advisor</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Keywords/Tags</th>\n",
       "      <th>GitHub Repository URL</th>\n",
       "      <th>Tools/Technologies Used</th>\n",
       "      <th>Project Outcome/Evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Institute</td>\n",
       "      <td>Daniel Kim</td>\n",
       "      <td>Fraud Detection System using Machine Learning</td>\n",
       "      <td>The project aims to develop a fraud detection ...</td>\n",
       "      <td>Finance, Machine Learning</td>\n",
       "      <td>Prof. Jessica Wong</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>Fraud Detection, Anomaly Detection, Transactio...</td>\n",
       "      <td>https://github.com/danielkim/fraud-detection-s...</td>\n",
       "      <td>Python, scikit-learn, Pandas</td>\n",
       "      <td>Achieved 98% accuracy in detecting fraudulent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI Robotics Research Center</td>\n",
       "      <td>Emily Liu</td>\n",
       "      <td>Humanoid Robot for Assisting Elderly People</td>\n",
       "      <td>The project aims to develop an advanced roboti...</td>\n",
       "      <td>Robotics, Artificial Intelligence</td>\n",
       "      <td>Dr. David Chen</td>\n",
       "      <td>2023-07-15</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>Humanoid Robot, Elderly Care, Natural Language...</td>\n",
       "      <td>https://github.com/emilyliu/humanoid-robot-ass...</td>\n",
       "      <td>ROS, TensorFlow, OpenCV</td>\n",
       "      <td>Achieved human-like interactions with elderly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning Lab</td>\n",
       "      <td>Jason Wang</td>\n",
       "      <td>Autonomous Vehicle Navigation using Deep Reinf...</td>\n",
       "      <td>The project focuses on developing deep reinfor...</td>\n",
       "      <td>Autonomous Vehicles, Deep Learning</td>\n",
       "      <td>Dr. Sarah Zhang</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>Autonomous Vehicles, Deep Reinforcement Learni...</td>\n",
       "      <td>https://github.com/jasonwang/autonomous-vehicl...</td>\n",
       "      <td>Python, TensorFlow, OpenAI Gym</td>\n",
       "      <td>Achieved safe and efficient navigation in vari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computer Vision Research Group</td>\n",
       "      <td>Sophia Chen</td>\n",
       "      <td>Semantic Segmentation for Medical Image Analysis</td>\n",
       "      <td>The project aims to develop a semantic segment...</td>\n",
       "      <td>Medical Imaging, Computer Vision</td>\n",
       "      <td>Prof. Michael Li</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>Semantic Segmentation, Medical Imaging, Deep L...</td>\n",
       "      <td>https://github.com/sophiachen/medical-image-se...</td>\n",
       "      <td>Python, TensorFlow, PyTorch</td>\n",
       "      <td>Achieved state-of-the-art performance in seman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Natural Language Processing Lab</td>\n",
       "      <td>Ryan Patel</td>\n",
       "      <td>Emotion Recognition from Text using Deep Learning</td>\n",
       "      <td>The project aims to develop a deep learning-ba...</td>\n",
       "      <td>Natural Language Processing, Deep Learning</td>\n",
       "      <td>Dr. Michelle Chen</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>Emotion Recognition, Sentiment Analysis, Text ...</td>\n",
       "      <td>https://github.com/ryanpatel/emotion-recognition</td>\n",
       "      <td>Python, TensorFlow, PyTorch, NLTK</td>\n",
       "      <td>Achieved state-of-the-art performance in emoti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   University Name Student Name  \\\n",
       "0           Data Science Institute   Daniel Kim   \n",
       "1      AI Robotics Research Center    Emily Liu   \n",
       "2             Machine Learning Lab   Jason Wang   \n",
       "3   Computer Vision Research Group  Sophia Chen   \n",
       "4  Natural Language Processing Lab   Ryan Patel   \n",
       "\n",
       "                                       Project Title  \\\n",
       "0      Fraud Detection System using Machine Learning   \n",
       "1        Humanoid Robot for Assisting Elderly People   \n",
       "2  Autonomous Vehicle Navigation using Deep Reinf...   \n",
       "3   Semantic Segmentation for Medical Image Analysis   \n",
       "4  Emotion Recognition from Text using Deep Learning   \n",
       "\n",
       "                                 Project Description  \\\n",
       "0  The project aims to develop a fraud detection ...   \n",
       "1  The project aims to develop an advanced roboti...   \n",
       "2  The project focuses on developing deep reinfor...   \n",
       "3  The project aims to develop a semantic segment...   \n",
       "4  The project aims to develop a deep learning-ba...   \n",
       "\n",
       "                       Project Category/Field Project Supervisor/Advisor  \\\n",
       "0                   Finance, Machine Learning         Prof. Jessica Wong   \n",
       "1           Robotics, Artificial Intelligence             Dr. David Chen   \n",
       "2          Autonomous Vehicles, Deep Learning            Dr. Sarah Zhang   \n",
       "3            Medical Imaging, Computer Vision           Prof. Michael Li   \n",
       "4  Natural Language Processing, Deep Learning          Dr. Michelle Chen   \n",
       "\n",
       "   Start Date    End Date                                      Keywords/Tags  \\\n",
       "0  2023-06-01  2024-03-01  Fraud Detection, Anomaly Detection, Transactio...   \n",
       "1  2023-07-15  2024-04-15  Humanoid Robot, Elderly Care, Natural Language...   \n",
       "2  2023-08-01  2024-05-01  Autonomous Vehicles, Deep Reinforcement Learni...   \n",
       "3  2023-09-15  2024-06-15  Semantic Segmentation, Medical Imaging, Deep L...   \n",
       "4  2023-10-01  2024-07-01  Emotion Recognition, Sentiment Analysis, Text ...   \n",
       "\n",
       "                               GitHub Repository URL  \\\n",
       "0  https://github.com/danielkim/fraud-detection-s...   \n",
       "1  https://github.com/emilyliu/humanoid-robot-ass...   \n",
       "2  https://github.com/jasonwang/autonomous-vehicl...   \n",
       "3  https://github.com/sophiachen/medical-image-se...   \n",
       "4   https://github.com/ryanpatel/emotion-recognition   \n",
       "\n",
       "             Tools/Technologies Used  \\\n",
       "0       Python, scikit-learn, Pandas   \n",
       "1            ROS, TensorFlow, OpenCV   \n",
       "2     Python, TensorFlow, OpenAI Gym   \n",
       "3        Python, TensorFlow, PyTorch   \n",
       "4  Python, TensorFlow, PyTorch, NLTK   \n",
       "\n",
       "                          Project Outcome/Evaluation  \n",
       "0  Achieved 98% accuracy in detecting fraudulent ...  \n",
       "1  Achieved human-like interactions with elderly ...  \n",
       "2  Achieved safe and efficient navigation in vari...  \n",
       "3  Achieved state-of-the-art performance in seman...  \n",
       "4  Achieved state-of-the-art performance in emoti...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "projects=pd.read_csv(\"output.csv\")\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11028959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Project Category/Field</th>\n",
       "      <th>Keywords/Tags</th>\n",
       "      <th>Tools/Technologies Used</th>\n",
       "      <th>Project Outcome/Evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fraud Detection System using Machine Learning</td>\n",
       "      <td>The project aims to develop a fraud detection ...</td>\n",
       "      <td>Finance, Machine Learning</td>\n",
       "      <td>Fraud Detection, Anomaly Detection, Transactio...</td>\n",
       "      <td>Python, scikit-learn, Pandas</td>\n",
       "      <td>Achieved 98% accuracy in detecting fraudulent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Humanoid Robot for Assisting Elderly People</td>\n",
       "      <td>The project aims to develop an advanced roboti...</td>\n",
       "      <td>Robotics, Artificial Intelligence</td>\n",
       "      <td>Humanoid Robot, Elderly Care, Natural Language...</td>\n",
       "      <td>ROS, TensorFlow, OpenCV</td>\n",
       "      <td>Achieved human-like interactions with elderly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autonomous Vehicle Navigation using Deep Reinf...</td>\n",
       "      <td>The project focuses on developing deep reinfor...</td>\n",
       "      <td>Autonomous Vehicles, Deep Learning</td>\n",
       "      <td>Autonomous Vehicles, Deep Reinforcement Learni...</td>\n",
       "      <td>Python, TensorFlow, OpenAI Gym</td>\n",
       "      <td>Achieved safe and efficient navigation in vari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Semantic Segmentation for Medical Image Analysis</td>\n",
       "      <td>The project aims to develop a semantic segment...</td>\n",
       "      <td>Medical Imaging, Computer Vision</td>\n",
       "      <td>Semantic Segmentation, Medical Imaging, Deep L...</td>\n",
       "      <td>Python, TensorFlow, PyTorch</td>\n",
       "      <td>Achieved state-of-the-art performance in seman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emotion Recognition from Text using Deep Learning</td>\n",
       "      <td>The project aims to develop a deep learning-ba...</td>\n",
       "      <td>Natural Language Processing, Deep Learning</td>\n",
       "      <td>Emotion Recognition, Sentiment Analysis, Text ...</td>\n",
       "      <td>Python, TensorFlow, PyTorch, NLTK</td>\n",
       "      <td>Achieved state-of-the-art performance in emoti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Project Title  \\\n",
       "0      Fraud Detection System using Machine Learning   \n",
       "1        Humanoid Robot for Assisting Elderly People   \n",
       "2  Autonomous Vehicle Navigation using Deep Reinf...   \n",
       "3   Semantic Segmentation for Medical Image Analysis   \n",
       "4  Emotion Recognition from Text using Deep Learning   \n",
       "\n",
       "                                 Project Description  \\\n",
       "0  The project aims to develop a fraud detection ...   \n",
       "1  The project aims to develop an advanced roboti...   \n",
       "2  The project focuses on developing deep reinfor...   \n",
       "3  The project aims to develop a semantic segment...   \n",
       "4  The project aims to develop a deep learning-ba...   \n",
       "\n",
       "                       Project Category/Field  \\\n",
       "0                   Finance, Machine Learning   \n",
       "1           Robotics, Artificial Intelligence   \n",
       "2          Autonomous Vehicles, Deep Learning   \n",
       "3            Medical Imaging, Computer Vision   \n",
       "4  Natural Language Processing, Deep Learning   \n",
       "\n",
       "                                       Keywords/Tags  \\\n",
       "0  Fraud Detection, Anomaly Detection, Transactio...   \n",
       "1  Humanoid Robot, Elderly Care, Natural Language...   \n",
       "2  Autonomous Vehicles, Deep Reinforcement Learni...   \n",
       "3  Semantic Segmentation, Medical Imaging, Deep L...   \n",
       "4  Emotion Recognition, Sentiment Analysis, Text ...   \n",
       "\n",
       "             Tools/Technologies Used  \\\n",
       "0       Python, scikit-learn, Pandas   \n",
       "1            ROS, TensorFlow, OpenCV   \n",
       "2     Python, TensorFlow, OpenAI Gym   \n",
       "3        Python, TensorFlow, PyTorch   \n",
       "4  Python, TensorFlow, PyTorch, NLTK   \n",
       "\n",
       "                          Project Outcome/Evaluation  \n",
       "0  Achieved 98% accuracy in detecting fraudulent ...  \n",
       "1  Achieved human-like interactions with elderly ...  \n",
       "2  Achieved safe and efficient navigation in vari...  \n",
       "3  Achieved state-of-the-art performance in seman...  \n",
       "4  Achieved state-of-the-art performance in emoti...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects=projects[[\"Project Title\",\"Project Description\",\"Project Category/Field\",\"Keywords/Tags\",\"Tools/Technologies Used\",\"Project Outcome/Evaluation\"]]\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0804ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "190    False\n",
       "191    False\n",
       "192    False\n",
       "193    False\n",
       "194    False\n",
       "Length: 195, dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1c4057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136ce52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects=projects.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b8d002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "190    False\n",
       "191    False\n",
       "192    False\n",
       "193    False\n",
       "194    False\n",
       "Length: 189, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7df627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6b056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects['tags']=projects['Project Description']+projects['Project Category/Field']+projects['Keywords/Tags']+projects['Tools/Technologies Used']+projects['Project Outcome/Evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba5aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=projects[['Project Title','tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9647ce80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fraud Detection System using Machine Learning</td>\n",
       "      <td>The project aims to develop a fraud detection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Humanoid Robot for Assisting Elderly People</td>\n",
       "      <td>The project aims to develop an advanced roboti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autonomous Vehicle Navigation using Deep Reinf...</td>\n",
       "      <td>The project focuses on developing deep reinfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Semantic Segmentation for Medical Image Analysis</td>\n",
       "      <td>The project aims to develop a semantic segment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emotion Recognition from Text using Deep Learning</td>\n",
       "      <td>The project aims to develop a deep learning-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Facial Expression Recognition using Deep Learning</td>\n",
       "      <td>The project aims to recognize facial expressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Visual SLAM for Autonomous Navigation</td>\n",
       "      <td>The project aims to develop a visual simultane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Scene Understanding using Semantic Segmentation</td>\n",
       "      <td>The project aims to understand visual scenes b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Visual Object Tracking using Siamese Networks</td>\n",
       "      <td>The project aims to track objects across conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Lesion Detection in Dermoscopy Images using De...</td>\n",
       "      <td>The project aims to detect skin lesions in der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Project Title  \\\n",
       "0        Fraud Detection System using Machine Learning   \n",
       "1          Humanoid Robot for Assisting Elderly People   \n",
       "2    Autonomous Vehicle Navigation using Deep Reinf...   \n",
       "3     Semantic Segmentation for Medical Image Analysis   \n",
       "4    Emotion Recognition from Text using Deep Learning   \n",
       "..                                                 ...   \n",
       "190  Facial Expression Recognition using Deep Learning   \n",
       "191              Visual SLAM for Autonomous Navigation   \n",
       "192    Scene Understanding using Semantic Segmentation   \n",
       "193      Visual Object Tracking using Siamese Networks   \n",
       "194  Lesion Detection in Dermoscopy Images using De...   \n",
       "\n",
       "                                                  tags  \n",
       "0    The project aims to develop a fraud detection ...  \n",
       "1    The project aims to develop an advanced roboti...  \n",
       "2    The project focuses on developing deep reinfor...  \n",
       "3    The project aims to develop a semantic segment...  \n",
       "4    The project aims to develop a deep learning-ba...  \n",
       "..                                                 ...  \n",
       "190  The project aims to recognize facial expressio...  \n",
       "191  The project aims to develop a visual simultane...  \n",
       "192  The project aims to understand visual scenes b...  \n",
       "193  The project aims to track objects across conse...  \n",
       "194  The project aims to detect skin lesions in der...  \n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d4ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fraud Detection System using Machine Learning</td>\n",
       "      <td>The project aims to develop a fraud detection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Humanoid Robot for Assisting Elderly People</td>\n",
       "      <td>The project aims to develop an advanced roboti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Autonomous Vehicle Navigation using Deep Reinf...</td>\n",
       "      <td>The project focuses on developing deep reinfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Semantic Segmentation for Medical Image Analysis</td>\n",
       "      <td>The project aims to develop a semantic segment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emotion Recognition from Text using Deep Learning</td>\n",
       "      <td>The project aims to develop a deep learning-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>185</td>\n",
       "      <td>Facial Expression Recognition using Deep Learning</td>\n",
       "      <td>The project aims to recognize facial expressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>186</td>\n",
       "      <td>Visual SLAM for Autonomous Navigation</td>\n",
       "      <td>The project aims to develop a visual simultane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>187</td>\n",
       "      <td>Scene Understanding using Semantic Segmentation</td>\n",
       "      <td>The project aims to understand visual scenes b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>188</td>\n",
       "      <td>Visual Object Tracking using Siamese Networks</td>\n",
       "      <td>The project aims to track objects across conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>189</td>\n",
       "      <td>Lesion Detection in Dermoscopy Images using De...</td>\n",
       "      <td>The project aims to detect skin lesions in der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     project_id                                              title  \\\n",
       "0             1      Fraud Detection System using Machine Learning   \n",
       "1             2        Humanoid Robot for Assisting Elderly People   \n",
       "2             3  Autonomous Vehicle Navigation using Deep Reinf...   \n",
       "3             4   Semantic Segmentation for Medical Image Analysis   \n",
       "4             5  Emotion Recognition from Text using Deep Learning   \n",
       "..          ...                                                ...   \n",
       "190         185  Facial Expression Recognition using Deep Learning   \n",
       "191         186              Visual SLAM for Autonomous Navigation   \n",
       "192         187    Scene Understanding using Semantic Segmentation   \n",
       "193         188      Visual Object Tracking using Siamese Networks   \n",
       "194         189  Lesion Detection in Dermoscopy Images using De...   \n",
       "\n",
       "                                                  tags  \n",
       "0    The project aims to develop a fraud detection ...  \n",
       "1    The project aims to develop an advanced roboti...  \n",
       "2    The project focuses on developing deep reinfor...  \n",
       "3    The project aims to develop a semantic segment...  \n",
       "4    The project aims to develop a deep learning-ba...  \n",
       "..                                                 ...  \n",
       "190  The project aims to recognize facial expressio...  \n",
       "191  The project aims to develop a visual simultane...  \n",
       "192  The project aims to understand visual scenes b...  \n",
       "193  The project aims to track objects across conse...  \n",
       "194  The project aims to detect skin lesions in der...  \n",
       "\n",
       "[189 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects['project_id'] = range(1, len(projects) + 1)\n",
    "projects['title']=projects['Project Title']\n",
    "new_df = projects[['project_id', 'title', 'tags']]\n",
    "new_df = new_df[['project_id', 'title', 'tags']]\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63179975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all to lower string\n",
    "new_df['tags']=new_df['tags'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3417ee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fraud Detection System using Machine Learning</td>\n",
       "      <td>the project aims to develop a fraud detection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Humanoid Robot for Assisting Elderly People</td>\n",
       "      <td>the project aims to develop an advanced roboti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Autonomous Vehicle Navigation using Deep Reinf...</td>\n",
       "      <td>the project focuses on developing deep reinfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Semantic Segmentation for Medical Image Analysis</td>\n",
       "      <td>the project aims to develop a semantic segment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emotion Recognition from Text using Deep Learning</td>\n",
       "      <td>the project aims to develop a deep learning-ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   project_id                                              title  \\\n",
       "0           1      Fraud Detection System using Machine Learning   \n",
       "1           2        Humanoid Robot for Assisting Elderly People   \n",
       "2           3  Autonomous Vehicle Navigation using Deep Reinf...   \n",
       "3           4   Semantic Segmentation for Medical Image Analysis   \n",
       "4           5  Emotion Recognition from Text using Deep Learning   \n",
       "\n",
       "                                                tags  \n",
       "0  the project aims to develop a fraud detection ...  \n",
       "1  the project aims to develop an advanced roboti...  \n",
       "2  the project focuses on developing deep reinfor...  \n",
       "3  the project aims to develop a semantic segment...  \n",
       "4  the project aims to develop a deep learning-ba...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa06b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The term \"scikit\" in \"scikit-learn\" is derived from the word \"SciKit,\" where \"Sci\" stands for science or scientific, and \"Kit\" refers to a collection of tools or a toolkit. In the context of scikit-learn, it essentially means a scientific toolkit for machine learning in Python.\n",
    "#CountVectorizer is a class in scikit-learn, specifically in the sklearn.feature_extraction.text module. It is used for converting a collection of text documents to a matrix of token counts.\n",
    "#Here's a brief overview of how CountVectorizer works:\n",
    "#Tokenization: It tokenizes the input text, which means breaking down the text into individual words or terms (tokens).\n",
    "#Counting: It counts the occurrences of each token in each document.\n",
    "#Vectorization: It represents each document as a vector, where each element of the vector corresponds to the count of a particular token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d25024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before that we have to do stemming\n",
    "#stemming ['loveed','loving','love']====>['love','love','love']\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dcae20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    list=[]\n",
    "    for i in text.split():\n",
    "        list.append(ps.stem(i))\n",
    "    return \" \".join(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d87df130",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags']=new_df['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92c9f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words removes stopwords\n",
    "#max_features says how many words to select\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=2000,stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478b2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The toarray() method is used after fit_transform when working with sparse matrices in scikit-learn. Many text-related transformations, such as those performed by CountVectorizer or TfidfVectorizer, result in sparse matrices by default. Sparse matrices are a memory-efficient way to represent matrices that are mostly composed of zeros.\n",
    "#Sparse Matrices:In natural language processing (NLP), especially when dealing with large text corpora, the term-document matrix can be very large and sparse (most entries are zero). Representing such matrices in a dense form (using a regular NumPy array) would be memory-inefficient.\n",
    "#Memory Efficiency:The fit_transform method, when used with text transformers, often returns a sparse matrix by default to save memory. A sparse matrix only stores non-zero elements and their positions, which can be much more memory-efficient when dealing with large datasets.\n",
    "#The fit_transform method in scikit-learn is commonly used for two purposes:Fit:Learning from Data: During the \"fit\" step, the transformer (in this case, CountVectorizer) analyzes the input data to learn any necessary parameters. For CountVectorizer, this involves learning the vocabulary (unique words) from the training data.Transform:\n",
    "#Applying the Transformation: The \"transform\" step applies the learned parameters to the input data, producing the transformed output. In the case of CountVectorizer, it converts a collection of text documents into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7a02977",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=cv.fit_transform(new_df['tags']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b460f18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3218637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2d', '3d', 'ab', ..., 'yolo', 'zero', 'zk'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv method get_feature_names gives the most common 5000 words\n",
    "cv.get_feature_names_out()\n",
    "#In scikit-learn versions 0.22.0 and later, get_feature_names is available as get_feature_names_out. The name of the method was changed in the later versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54cc473c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2d', '3d', 'ab', 'abil', 'abilities', 'abnorm', 'absolut',\n",
       "       'abstract', 'academ', 'acceler', 'acceleration', 'accept',\n",
       "       'access', 'accord', 'account', 'accur', 'accuraci', 'accuracy',\n",
       "       'achiev', 'acid', 'acoust', 'acquir', 'acquisit', 'acquisition',\n",
       "       'act', 'action', 'actions', 'activ', 'activities', 'activity',\n",
       "       'actor', 'actuat', 'ad', 'adam', 'adapt', 'adaptation', 'add',\n",
       "       'addit', 'address', 'adher', 'adjust', 'admet', 'administr',\n",
       "       'adopt', 'advanc', 'advers', 'adversari', 'aerial', 'affect',\n",
       "       'affin', 'ag', 'agencies', 'agent', 'agents', 'aggreg',\n",
       "       'aggregation', 'agi', 'agil', 'agnost', 'agreement', 'agriculture',\n",
       "       'ai', 'aid', 'aim', 'air', 'alert', 'alerting', 'alerts',\n",
       "       'algorithm', 'algorithms', 'align', 'allel', 'alloc', 'allocation',\n",
       "       'allow', 'amino', 'analys', 'analyses', 'analysi', 'analysis',\n",
       "       'analysissenti', 'analyt', 'analytics', 'analyticspython',\n",
       "       'analyz', 'analyze', 'anatom', 'anger', 'ani', 'anim', 'animation',\n",
       "       'animations', 'annot', 'annotation', 'annotations', 'anomal',\n",
       "       'anomali', 'anomalies', 'anonym', 'anonymity', 'answer',\n",
       "       'answering', 'anticip', 'antimicrobi', 'api', 'apis', 'app',\n",
       "       'appear', 'appli', 'applic', 'applications', 'approach',\n",
       "       'approaches', 'appropri', 'ar', 'architectur', 'architecture',\n",
       "       'architectures', 'area', 'areas', 'arima', 'art', 'articl',\n",
       "       'articles', 'artifacts', 'artifici', 'artist', 'asd', 'aspect',\n",
       "       'assembl', 'assert', 'assess', 'assessment', 'assessments',\n",
       "       'asset', 'assets', 'assign', 'assignment', 'assist', 'assistants',\n",
       "       'associ', 'atmospher', 'attack', 'attacks', 'attent', 'attention',\n",
       "       'attitudes', 'attribut', 'attributes', 'auc', 'auction', 'audio',\n",
       "       'audit', 'augment', 'augmentation', 'authent', 'authentication',\n",
       "       'author', 'authorities', 'autoencod', 'autom', 'automat',\n",
       "       'automation', 'autonom', 'autonomi', 'autonomously', 'autoregress',\n",
       "       'averag', 'avoid', 'avoidance', 'awar', 'backbon', 'backend',\n",
       "       'backpropag', 'bacteri', 'bag', 'balanc', 'bandwidth', 'bas',\n",
       "       'base', 'bases', 'batch', 'bayesian', 'bci', 'bcis', 'befor',\n",
       "       'behavior', 'behaviors', 'benchmark', 'benchmarking', 'benefit',\n",
       "       'bert', 'best', 'bia', 'bidirect', 'bilingu', 'binari', 'bind',\n",
       "       'bioinformat', 'bioinformatics', 'biolog', 'biology', 'biomark',\n",
       "       'biomarkers', 'biomed', 'biopythonfocus', 'black', 'bleu',\n",
       "       'blockchain', 'blockchainsolidity', 'bodi', 'boltzmann', 'boost',\n",
       "       'boosting', 'bootstrapping', 'borrow', 'borrowing', 'bound',\n",
       "       'boundari', 'boundaries', 'box', 'bptt', 'brain', 'brand',\n",
       "       'breach', 'brush', 'build', 'buildings', 'bundl', 'busi',\n",
       "       'calendar', 'calibr', 'calibration', 'calling', 'camera',\n",
       "       'cameras', 'candid', 'capabilities', 'capabl', 'captur', 'care',\n",
       "       'carlini', 'carlo', 'case', 'cases', 'categori', 'categories',\n",
       "       'caus', 'cdss', 'cellular', 'cent', 'centr', 'central', 'chain',\n",
       "       'challeng', 'chang', 'change', 'changes', 'channel', 'character',\n",
       "       'characterist', 'characteristics', 'charts', 'chat', 'chatbot',\n",
       "       'chatbots', 'chemic', 'children', 'chronic', 'circuit', 'citizen',\n",
       "       'class', 'classic', 'classif', 'classifi', 'classification',\n",
       "       'classifiers', 'clean', 'cleaning', 'clim', 'climat', 'clinic',\n",
       "       'clinician', 'clinicians', 'closed', 'closur', 'cloud', 'clouds',\n",
       "       'cluster', 'clustering', 'clutter', 'cnn', 'cnns', 'cnnspython',\n",
       "       'code', 'codes', 'coefficient', 'cognit', 'coher', 'coherence',\n",
       "       'cohort', 'collabor', 'collaboration', 'collater', 'collect',\n",
       "       'collection', 'collis', 'collision', 'collisions', 'color',\n",
       "       'combin', 'comfort', 'command', 'commands', 'comments', 'common',\n",
       "       'commun', 'communication', 'communities', 'community', 'compar',\n",
       "       'compil', 'complementari', 'complet', 'completion', 'complex',\n",
       "       'complexity', 'complianc', 'compon', 'components', 'composition',\n",
       "       'compound', 'comprehens', 'compromis', 'comput', 'computation',\n",
       "       'computing', 'concept', 'concepts', 'concis', 'condit',\n",
       "       'conditions', 'conduct', 'confid', 'confidenti', 'confidentiality',\n",
       "       'configur', 'confus', 'congest', 'connect', 'connections',\n",
       "       'consciou', 'conscious', 'consecut', 'consensu', 'consensus',\n",
       "       'conserv', 'consid', 'consist', 'consistency', 'constrain',\n",
       "       'constraint', 'constraints', 'construct', 'consumpt',\n",
       "       'consumption', 'contain', 'content', 'context', 'contextu',\n",
       "       'continu', 'contract', 'contracts', 'contrast', 'contribut',\n",
       "       'control', 'controllers', 'converg', 'convers', 'conversations',\n",
       "       'convey', 'convolut', 'cooper', 'coordin', 'coordination',\n",
       "       'corpora', 'corpu', 'correct', 'correction', 'correl',\n",
       "       'correspond', 'countermeasur', 'cover', 'coverag', 'cp', 'craft',\n",
       "       'creat', 'creation', 'creativ', 'credenti', 'crit', 'criteria',\n",
       "       'crop', 'cross', 'cryptocurr', 'cryptograph', 'cryptography', 'ct',\n",
       "       'cue', 'cues', 'current', 'curriculum', 'curv', 'custom',\n",
       "       'customers', 'cyber', 'cybersecur', 'cybersecurity',\n",
       "       'cybersecuritypython', 'cyclists', 'daili', 'dashboard',\n",
       "       'dashboards', 'data', 'databas', 'databases', 'dataset',\n",
       "       'datasets', 'decentr', 'decis', 'decision', 'decisions', 'decod',\n",
       "       'decomposit', 'deep', 'deeplab', 'defens', 'defense', 'defenses',\n",
       "       'defi', 'defin', 'deletions', 'delin', 'deliveri', 'delivery',\n",
       "       'demand', 'demograph', 'demographics', 'demonstr', 'deni',\n",
       "       'denoising', 'dens', 'depend', 'deploy', 'deployment', 'depth',\n",
       "       'deriv', 'dermoscopi', 'descent', 'descriptor', 'design', 'desir',\n",
       "       'detect', 'detection', 'detectionobject', 'detector', 'determin',\n",
       "       'develop', 'development', 'deviat', 'devic', 'devices', 'dex',\n",
       "       'diagnos', 'diagnosi', 'diagnosis', 'diagnost', 'dialog',\n",
       "       'dialogu', 'dice', 'differ', 'differenti', 'digit', 'dimension',\n",
       "       'direct', 'directli', 'disast', 'discov', 'discoveri', 'discovery',\n",
       "       'discrimin', 'diseas', 'diseases', 'disord', 'display', 'displays',\n",
       "       'distanc', 'distance', 'distil', 'distillation'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71c43c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1604629",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity=cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26e826aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 189)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a64fbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.25159068, 0.29600731, 0.28905527, 0.48266934,\n",
       "       0.48834164, 0.32269672, 0.1628344 , 0.22787303, 0.11803709,\n",
       "       0.19544506, 0.18092155, 0.12094906, 0.28090818, 0.36623994,\n",
       "       0.34707938, 0.43218697, 0.07943424, 0.17606631, 0.23649371,\n",
       "       0.20367562, 0.44448981, 0.29518331, 0.37538565, 0.40274209,\n",
       "       0.11038266, 0.38097186, 0.22547802, 0.23854286, 0.16922139,\n",
       "       0.35993828, 0.24488301, 0.23152225, 0.15071857, 0.49379442,\n",
       "       0.2267368 , 0.25470451, 0.27084254, 0.40155524, 0.18469402,\n",
       "       0.20034248, 0.18839736, 0.2210511 , 0.1185386 , 0.22700348,\n",
       "       0.22610697, 0.34553188, 0.22805713, 0.21502411, 0.20872744,\n",
       "       0.44268941, 0.37477829, 0.20082219, 0.17560302, 0.08918377,\n",
       "       0.21485335, 0.2031018 , 0.36284919, 0.30656087, 0.24726035,\n",
       "       0.35323971, 0.3247179 , 0.47313853, 0.16235895, 0.26664642,\n",
       "       0.52724456, 0.2073658 , 0.1912982 , 0.0911974 , 0.15194633,\n",
       "       0.22515404, 0.14531934, 0.20038637, 0.11989408, 0.44352305,\n",
       "       0.46350893, 0.17577775, 0.24266683, 0.15312107, 0.08683303,\n",
       "       0.11927055, 0.23847597, 0.26993191, 0.25856523, 0.12270419,\n",
       "       0.15062893, 0.08666286, 0.52054612, 0.10285571, 0.23247141,\n",
       "       0.19269966, 0.35451176, 0.10841247, 0.14040648, 0.25152608,\n",
       "       0.23663868, 0.16096297, 0.12782817, 0.18201497, 0.28486293,\n",
       "       0.18231228, 0.06498843, 0.21420612, 0.30663208, 0.29683116,\n",
       "       0.18851757, 0.30048229, 0.40342639, 0.16570709, 0.44500598,\n",
       "       0.29850643, 0.37386488, 0.18836496, 0.30298005, 0.15342222,\n",
       "       0.41579839, 0.25257294, 0.21858947, 0.44514265, 0.36295732,\n",
       "       0.0711886 , 0.1521388 , 0.23468382, 0.20179297, 0.32631226,\n",
       "       0.22720566, 0.1764467 , 0.43484658, 0.20809634, 0.13198451,\n",
       "       0.18777237, 0.36664023, 0.36668432, 0.46168026, 0.28125398,\n",
       "       0.61314968, 0.33738724, 0.39812172, 0.3991264 , 0.38805976,\n",
       "       0.23681378, 0.59779507, 0.59194595, 0.60463568, 0.5709588 ,\n",
       "       0.54478109, 0.61011786, 0.6013463 , 0.60803521, 0.49427803,\n",
       "       0.57816427, 0.44050968, 0.54185652, 0.59095721, 0.56439606,\n",
       "       0.53348262, 0.61986771, 0.4623657 , 0.54050643, 0.46813596,\n",
       "       0.55211043, 0.61983598, 0.11035753, 0.17019042, 0.15321772,\n",
       "       0.10238429, 0.12485529, 0.15732679, 0.05961588, 0.10191368,\n",
       "       0.12577   , 0.12833779, 0.19053427, 0.07129877, 0.07791937,\n",
       "       0.08409694, 0.08425789, 0.1237543 , 0.06383499, 0.12864067,\n",
       "       0.1381975 , 0.26729102, 0.31771902, 0.12600139, 0.10456293,\n",
       "       0.0461205 , 0.09356015, 0.09942991, 0.15124754])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1db71a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x2a943b3fa00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=enumerate(similarity[0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c575bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9999999999999999),\n",
       " (1, 0.2515906841288554),\n",
       " (2, 0.29600730956978877),\n",
       " (3, 0.2890552677367107),\n",
       " (4, 0.4826693422019439),\n",
       " (5, 0.48834163572588757),\n",
       " (6, 0.3226967229600173),\n",
       " (7, 0.16283440323598433),\n",
       " (8, 0.2278730282008954),\n",
       " (9, 0.11803708870426109),\n",
       " (10, 0.19544506218346694),\n",
       " (11, 0.18092154970993055),\n",
       " (12, 0.12094906140482449),\n",
       " (13, 0.28090818442083015),\n",
       " (14, 0.36623993685912143),\n",
       " (15, 0.34707937813820144),\n",
       " (16, 0.4321869651146186),\n",
       " (17, 0.0794342416486011),\n",
       " (18, 0.17606631093461034),\n",
       " (19, 0.23649370517484933),\n",
       " (20, 0.20367561877204987),\n",
       " (21, 0.44448981032864965),\n",
       " (22, 0.29518330782307634),\n",
       " (23, 0.3753856541315976),\n",
       " (24, 0.4027420901222941),\n",
       " (25, 0.11038265759067317),\n",
       " (26, 0.38097185699514463),\n",
       " (27, 0.22547801696496667),\n",
       " (28, 0.23854286487637524),\n",
       " (29, 0.16922139087719743),\n",
       " (30, 0.3599382801838737),\n",
       " (31, 0.2448830132164534),\n",
       " (32, 0.2315222497557501),\n",
       " (33, 0.15071856698686403),\n",
       " (34, 0.4937944186683415),\n",
       " (35, 0.22673680143916194),\n",
       " (36, 0.2547045096870135),\n",
       " (37, 0.2708425414189378),\n",
       " (38, 0.4015552421101293),\n",
       " (39, 0.1846940173533948),\n",
       " (40, 0.20034247546281536),\n",
       " (41, 0.18839736477543656),\n",
       " (42, 0.22105109564426823),\n",
       " (43, 0.11853859875364803),\n",
       " (44, 0.22700347714700292),\n",
       " (45, 0.2261069666301077),\n",
       " (46, 0.3455318761835568),\n",
       " (47, 0.2280571306411329),\n",
       " (48, 0.21502411294470924),\n",
       " (49, 0.2087274384725438),\n",
       " (50, 0.4426894079971203),\n",
       " (51, 0.3747782940816909),\n",
       " (52, 0.20082218512268227),\n",
       " (53, 0.17560301511730583),\n",
       " (54, 0.08918376927381888),\n",
       " (55, 0.21485335124885693),\n",
       " (56, 0.20310180107535894),\n",
       " (57, 0.36284919040785524),\n",
       " (58, 0.3065608727821305),\n",
       " (59, 0.24726034500457325),\n",
       " (60, 0.3532397140720662),\n",
       " (61, 0.3247178970243806),\n",
       " (62, 0.4731385330248599),\n",
       " (63, 0.16235894851219027),\n",
       " (64, 0.26664642342632394),\n",
       " (65, 0.5272445589876613),\n",
       " (66, 0.20736580206211372),\n",
       " (67, 0.19129820318340357),\n",
       " (68, 0.09119740273495465),\n",
       " (69, 0.15194633300583338),\n",
       " (70, 0.22515403555366828),\n",
       " (71, 0.14531934160258358),\n",
       " (72, 0.20038637239825852),\n",
       " (73, 0.11989407769474067),\n",
       " (74, 0.44352304775302026),\n",
       " (75, 0.46350893099650664),\n",
       " (76, 0.17577774869553134),\n",
       " (77, 0.24266683176761608),\n",
       " (78, 0.15312107368971736),\n",
       " (79, 0.08683303079902256),\n",
       " (80, 0.11927055485908117),\n",
       " (81, 0.2384759654664171),\n",
       " (82, 0.26993190554040164),\n",
       " (83, 0.25856522882824123),\n",
       " (84, 0.12270419262018888),\n",
       " (85, 0.15062893357603013),\n",
       " (86, 0.08666286428317317),\n",
       " (87, 0.5205461169919832),\n",
       " (88, 0.10285571013225926),\n",
       " (89, 0.2324714110633613),\n",
       " (90, 0.1926996576081472),\n",
       " (91, 0.35451176287970876),\n",
       " (92, 0.10841247201912167),\n",
       " (93, 0.14040647602011586),\n",
       " (94, 0.2515260780498569),\n",
       " (95, 0.23663868330493676),\n",
       " (96, 0.16096297471164442),\n",
       " (97, 0.12782816753357995),\n",
       " (98, 0.18201497324203136),\n",
       " (99, 0.28486293002533664),\n",
       " (100, 0.18231227521621554),\n",
       " (101, 0.06498843167890259),\n",
       " (102, 0.21420612221882815),\n",
       " (103, 0.3066320796917917),\n",
       " (104, 0.29683115987189085),\n",
       " (105, 0.18851757409979727),\n",
       " (106, 0.30048228602328386),\n",
       " (107, 0.40342638507519957),\n",
       " (108, 0.1657070859100687),\n",
       " (109, 0.44500597921767326),\n",
       " (110, 0.2985064347869582),\n",
       " (111, 0.3738648775639644),\n",
       " (112, 0.18836495619353724),\n",
       " (113, 0.30298005278701534),\n",
       " (114, 0.1534222238339042),\n",
       " (115, 0.4157983937653839),\n",
       " (116, 0.2525729397015837),\n",
       " (117, 0.21858946802823637),\n",
       " (118, 0.44514265064465347),\n",
       " (119, 0.3629573233735901),\n",
       " (120, 0.07118859880854757),\n",
       " (121, 0.15213879631373448),\n",
       " (122, 0.2346838231103045),\n",
       " (123, 0.20179296837324284),\n",
       " (124, 0.3263122606781728),\n",
       " (125, 0.22720566381882046),\n",
       " (126, 0.1764466996888607),\n",
       " (127, 0.43484657720763153),\n",
       " (128, 0.20809633908461375),\n",
       " (129, 0.13198451472535505),\n",
       " (130, 0.1877723736015787),\n",
       " (131, 0.36664023113419286),\n",
       " (132, 0.36668432091364933),\n",
       " (133, 0.46168025897418036),\n",
       " (134, 0.2812539788361287),\n",
       " (135, 0.6131496811955904),\n",
       " (136, 0.3373872366340979),\n",
       " (137, 0.3981217155093954),\n",
       " (138, 0.39912640229031443),\n",
       " (139, 0.38805976304243),\n",
       " (140, 0.23681377839895457),\n",
       " (141, 0.5977950682110309),\n",
       " (142, 0.591945950647729),\n",
       " (143, 0.6046356755810319),\n",
       " (144, 0.5709587998369028),\n",
       " (145, 0.5447810854901958),\n",
       " (146, 0.6101178648654995),\n",
       " (147, 0.6013463014124362),\n",
       " (148, 0.6080352138453796),\n",
       " (149, 0.4942780268081671),\n",
       " (150, 0.5781642651870539),\n",
       " (151, 0.44050968359192894),\n",
       " (152, 0.541856522920512),\n",
       " (153, 0.5909572141988885),\n",
       " (154, 0.5643960550111914),\n",
       " (155, 0.5334826232644696),\n",
       " (156, 0.6198677147669256),\n",
       " (157, 0.4623657024321424),\n",
       " (158, 0.5405064326250612),\n",
       " (159, 0.46813596099296556),\n",
       " (160, 0.5521104331310438),\n",
       " (161, 0.6198359842804896),\n",
       " (162, 0.11035753058718369),\n",
       " (163, 0.17019041512471209),\n",
       " (164, 0.15321771874526274),\n",
       " (165, 0.10238429338954814),\n",
       " (166, 0.12485529317311908),\n",
       " (167, 0.15732679005613104),\n",
       " (168, 0.059615880436570415),\n",
       " (169, 0.10191367695100631),\n",
       " (170, 0.12576999619408205),\n",
       " (171, 0.1283377895839496),\n",
       " (172, 0.1905342656040553),\n",
       " (173, 0.0712987719910831),\n",
       " (174, 0.07791937224739796),\n",
       " (175, 0.0840969355368056),\n",
       " (176, 0.08425788675685038),\n",
       " (177, 0.12375429709880854),\n",
       " (178, 0.06383498902951833),\n",
       " (179, 0.1286406726029821),\n",
       " (180, 0.13819749820569555),\n",
       " (181, 0.2672910153478103),\n",
       " (182, 0.31771902390518036),\n",
       " (183, 0.12600139102492514),\n",
       " (184, 0.10456292660552613),\n",
       " (185, 0.04612050347010681),\n",
       " (186, 0.09356014857063999),\n",
       " (187, 0.09942991232100196),\n",
       " (188, 0.15124753549550507)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a47a31f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(156, 0.6198677147669256),\n",
       " (161, 0.6198359842804896),\n",
       " (135, 0.6131496811955904),\n",
       " (146, 0.6101178648654995),\n",
       " (148, 0.6080352138453796)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(enumerate(similarity[0])),reverse=True,key=lambda x:x[1])[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9a8de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    movie_index=new_df[new_df['title']==movie].index[0]\n",
    "    distances=similarity[movie_index]\n",
    "    movies_list=sorted(list(enumerate(distances)),reverse=True,key=lambda x:x[1])[1:6]\n",
    "    \n",
    "    for i in movies_list:\n",
    "        print(new_df.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "693ef24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Image Segmentation using Convolutional Neural Networks\n",
      "Medical Image Segmentation using Convolutional Neural Networks\n",
      "Medical Image Segmentation using U-Net\n",
      "Medical Image Analysis for Disease Diagnosis and Prognosis\n",
      "Deep Learning Models for Medical Image Analysis\n"
     ]
    }
   ],
   "source": [
    "recommend('Semantic Segmentation for Medical Image Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49f01ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis for Social Media Data\n",
      "Speech Emotion Recognition using Recurrent Neural Networks\n",
      "Text Generation using Transformer Models\n",
      "Disease Prediction using Deep Neural Networks\n",
      "Predicting Patient Readmission using Deep Neural Networks\n"
     ]
    }
   ],
   "source": [
    "recommend('Emotion Recognition from Text using Deep Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3122d197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Detection in Network Traffic using Machine Learning\n",
      "Intrusion Detection System using Machine Learning\n",
      "Anomaly Detection in Cyber-Physical Systems using Machine Learning\n",
      "Anomaly Detection in Time Series Data with Deep Learning\n",
      "Fraud Detection System using Machine Learning\n"
     ]
    }
   ],
   "source": [
    "recommend(\"Anomaly Detection in Network Traffic using Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c482166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Machine Learning for Cyber Threat Detection\n",
      "Adversarial Machine Learning for Cyber Defense\n",
      "Adversarial Attacks and Defenses in Deep Learning Models\n",
      "Adversarial Attack and Defense in Deep Learning Systems\n",
      "Adversarial Attacks and Defenses in Deep Learning\n"
     ]
    }
   ],
   "source": [
    "recommend(\"Adversarial Machine Learning for Cyber Defense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3cd6b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Machine Learning for Cyber Defense\n",
      "Adversarial Machine Learning for Cyber Threat Detection\n",
      "Adversarial Machine Learning for Cyber Defense\n",
      "Adversarial Attacks and Defenses in Deep Learning Models\n",
      "Adversarial Attack and Defense in Deep Learning Systems\n",
      "Adversarial Attacks and Defenses in Deep Learning\n"
     ]
    }
   ],
   "source": [
    "x=input()\n",
    "recommend(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbe1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=input()\n",
    "recommend(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d439873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
